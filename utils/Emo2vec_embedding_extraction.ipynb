{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import fairseq\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "@dataclass\n",
    "class UserDirModule:\n",
    "    user_dir: str\n",
    "\n",
    "model_dir = '/home/legalalien/Documents/Jiawei/EmoTracjectory/emotion2vec/upstream'\n",
    "checkpoint_dir = '/home/legalalien/Documents/Jiawei/EmoTracjectory/emotion2vec/emotion2vec_base.pt'\n",
    "granularity = 'utterance'\n",
    "\n",
    "model_path = UserDirModule(model_dir)\n",
    "fairseq.utils.import_user_module(model_path)\n",
    "model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([checkpoint_dir])\n",
    "model = model[0]\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "def load_audio(file_path):\n",
    "    \"\"\"Load audio file and resample to 16000 Hz if necessary.\"\"\"\n",
    "    audio, sr = torchaudio.load(file_path)\n",
    "    if sr != 16000:\n",
    "        print(\"This audio is not 16kHz, resampling to 16kHz:\", file_path)\n",
    "        resampler = T.Resample(orig_freq=sr, new_freq=16000)\n",
    "        audio = resampler(audio)\n",
    "        sr = 16000\n",
    "    return audio, sr\n",
    "\n",
    "\n",
    "def process_data():\n",
    "    # parser = get_parser()\n",
    "    # args = parser.parse_args()\n",
    "    # print(args)\n",
    "    source_csv_file = '/home/legalalien/Documents/Jiawei/EmoTracjectory/data_pre/sorted_detailed_train.csv'\n",
    "    csv_file = pd.read_csv(source_csv_file)\n",
    "    # wavs = csv_file['Wav-path'].values\n",
    "    \n",
    "    target_pkl_file = '/home/legalalien/Documents/Jiawei/EmoTracjectory/data_pre/emo2vec_train.pkl'\n",
    "    # generate target npy file if it does not exist\n",
    "    results = []\n",
    "    \n",
    "    for index, row in tqdm(csv_file.iterrows(), total=len(csv_file)):\n",
    "        wav_path = row['Wav-path']\n",
    "        if not os.path.exists(wav_path):\n",
    "            print(\"File not found: {}\".format(wav_path))\n",
    "            continue\n",
    "        # audio = load_audio(wav_path)\n",
    "        audio = wav_path\n",
    "\n",
    "        if audio.endswith('.wav'):\n",
    "            wav, sr = load_audio(audio)\n",
    "            channel = sf.info(audio).channels\n",
    "\n",
    "            assert sr == 16e3, \"Sample rate should be 16kHz, but got {}in file {}\".format(sr, audio)\n",
    "            assert channel == 1, \"Channel should be 1, but got {} in file {}\".format(channel, audio)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            source = wav.float().cuda()\n",
    "            if task.cfg.normalize:\n",
    "                source = F.layer_norm(source, source.shape)\n",
    "            try:\n",
    "                feats = model.extract_features(source, padding_mask=None)\n",
    "                feats = feats['x'].squeeze(0).cpu().numpy()\n",
    "                if granularity == 'frame':\n",
    "                    feats = feats\n",
    "                elif granularity == 'utterance':\n",
    "                    feats = np.mean(feats, axis=0)\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown granularity: {}\".format(args.granularity))\n",
    "                results.append(feats)\n",
    "                # np.save(target_file, feats)\n",
    "            except:\n",
    "                Exception(\"Error in extracting features from {}\".format(audio))\n",
    "\n",
    "    results = np.array(results)\n",
    "    print(\"Extracted features shape: {}\".format(results.shape))\n",
    "    with open(target_pkl_file, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(\"Saved features to {}\".format(target_pkl_file))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data shape: (15084, 768)\n",
      "Loaded data type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Load pkl file\n",
    "import pickle\n",
    "\n",
    "file_path = '/home/legalalien/Documents/Jiawei/EmoTracjectory/data_pre/emo2vec_train.pkl'\n",
    "def load_pkl(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        print(\"Loaded data shape: {}\".format(data.shape))\n",
    "        print(\"Loaded data type: {}\".format(type(data)))\n",
    "    return data\n",
    "\n",
    "data = load_pkl(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiaweiEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
